{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CS224U 分散表現",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3P1cvu2hfPYJ",
        "colab_type": "code",
        "outputId": "a2c90a2a-17e0-4920-c695-d6524b865e80",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "   !pip install mecab-python3 -q"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[K     |████████████████████████████████| 17.1MB 235kB/s \n",
            "\u001b[?25h"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yIsQC9dOfaPP",
        "colab_type": "code",
        "outputId": "97140cc5-a430-4089-f5a7-14747eb79199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        }
      },
      "source": [
        "# https://www.rondhuit.com/download.html Livedoorコーパスに基づいて作成した。\n",
        "!gdown https://drive.google.com/uc?id=1O7Iym9RgyiiFGL-WZBiW_pwrhMmwcU_g"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading...\n",
            "From: https://drive.google.com/uc?id=1O7Iym9RgyiiFGL-WZBiW_pwrhMmwcU_g\n",
            "To: /content/train.tsv\n",
            "15.2MB [00:00, 57.3MB/s]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hDjNGpO2fquG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import random \n",
        "def seed_everything(seed: int):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "\n",
        "seed_everything(99)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gN2IhAOfeiD",
        "colab_type": "code",
        "outputId": "d77aef83-6a16-4d86-9467-2f9e3407c85c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        }
      },
      "source": [
        "train_df = pd.read_csv(\"train.tsv\",sep='\\t')\n",
        "train_df[\"category\"] = train_df.label.astype('category').cat.codes\n",
        "train_df.head()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "      <th>category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>大島優子がここからどう破滅していくのか？　『闇金ウシジマくん』特報解禁“闇金”という禁断の題...</td>\n",
              "      <td>movie-enter</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>インタビュー：クリスチャン・ベール「演じることができるのは役者だけ」公開当時、全米歴代2位と...</td>\n",
              "      <td>movie-enter</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ブラックマジックデザイン、HyperDeck SSD レコーダーに タイムコード、DNxHD...</td>\n",
              "      <td>kaden-channel</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>センター試験終了！　受験生ファンに眞鍋かをりが的確なアドバイスをしていた【話題】寒い週末、毎...</td>\n",
              "      <td>kaden-channel</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>彼女の香りは、柔軟剤の香り都内で一人暮らしをしているサナエさん（28歳・医療関連）は、同僚の...</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text           label  category\n",
              "0  大島優子がここからどう破滅していくのか？　『闇金ウシジマくん』特報解禁“闇金”という禁断の題...     movie-enter         4\n",
              "1  インタビュー：クリスチャン・ベール「演じることができるのは役者だけ」公開当時、全米歴代2位と...     movie-enter         4\n",
              "2  ブラックマジックデザイン、HyperDeck SSD レコーダーに タイムコード、DNxHD...   kaden-channel         2\n",
              "3  センター試験終了！　受験生ファンに眞鍋かをりが的確なアドバイスをしていた【話題】寒い週末、毎...   kaden-channel         2\n",
              "4  彼女の香りは、柔軟剤の香り都内で一人暮らしをしているサナエさん（28歳・医療関連）は、同僚の...  dokujo-tsushin         0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tZFHhF1tW7ZK",
        "colab_type": "text"
      },
      "source": [
        "# Baseline TF-IDF"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WAreVPLlfk-S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "import MeCab\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_aKBvF8Pfz2v",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "m = MeCab.Tagger(\"-Owakati\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0XEhWrQf1tI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_xs = train_df[:500][\"text\"].apply(lambda x: m.parse(x))\n",
        "train_ys = train_df[:500]['category']\n",
        "\n",
        "test_xs = train_df[2000:3000][\"text\"].apply(lambda x: m.parse(x))\n",
        "test_ys = train_df[2000:3000]['category']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BhhojIbSrNo4",
        "colab_type": "code",
        "outputId": "6c3f3d4e-fc1a-4548-80b9-9227be0b881a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 126
        }
      },
      "source": [
        "train_xs.head(5)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    大島 優子 が ここ から どう 破滅 し て いく の か ？ 　 『 闇 金 ウシジマ ...\n",
              "1    インタビュー ： クリスチャン ・ ベール 「 演じる こと が できる の は 役者 だけ...\n",
              "2    ブラックマジックデザイン 、 HyperDeck SSD レコーダー に タイム コード 、...\n",
              "3    センター 試験 終了 ！ 　 受験生 ファン に 眞 鍋 かをり が 的確 な アドバイス ...\n",
              "4    彼女 の 香り は 、 柔軟 剤 の 香り 都内 で 一人暮らし を し て いる サナエ ...\n",
              "Name: text, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HPI8V28Uf3M_",
        "colab_type": "code",
        "outputId": "1b5870d2-52ec-4cd4-8ee8-ea9953c22148",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "vectorizer = TfidfVectorizer()\n",
        "train_dev_xs_ = vectorizer.fit_transform(train_xs)\n",
        "test_xs_ = vectorizer.transform(test_xs)\n",
        "print(train_dev_xs_.shape)\n",
        "\n",
        "lsa = TruncatedSVD(300)\n",
        "train_dev_xs_ = lsa.fit_transform(train_dev_xs_)\n",
        "test_xs_ = lsa.transform(test_xs_)\n",
        "print(train_dev_xs_.shape)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(500, 19401)\n",
            "(500, 300)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMIwTvsqf4hR",
        "colab_type": "code",
        "outputId": "1832ac90-b035-462b-aa52-6e41634b4da0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "model = GradientBoostingClassifier(random_state=23)\n",
        "model.fit(train_dev_xs_, train_ys)\n",
        "print(classification_report(test_ys, model.predict(test_xs_)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.74      0.76      0.75       127\n",
            "           1       0.74      0.69      0.72       118\n",
            "           2       0.87      0.76      0.81       112\n",
            "           3       0.32      0.54      0.40        69\n",
            "           4       0.78      0.84      0.81       101\n",
            "           5       0.78      0.60      0.68       126\n",
            "           6       0.86      0.88      0.87       121\n",
            "           7       0.96      0.83      0.89       132\n",
            "           8       0.76      0.80      0.78        94\n",
            "\n",
            "    accuracy                           0.75      1000\n",
            "   macro avg       0.75      0.74      0.74      1000\n",
            "weighted avg       0.78      0.75      0.76      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmI9uYYkXA-m",
        "colab_type": "text"
      },
      "source": [
        "# FastText"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "unJerl3BXQQF",
        "colab_type": "text"
      },
      "source": [
        "## Default"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JCQdW1Egf_D8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from gensim.models.fasttext import FastText\n",
        "import numpy as np\n",
        "\n",
        "def sent2vec(model, sentence):\n",
        "    vector = np.zeros(model.vector_size)\n",
        "    for word in sentence:\n",
        "        if word in model.wv.vocab:\n",
        "            vector += model.wv.__getitem__(word)\n",
        "    return vector / len(sentence)\n",
        "    \n",
        "paser = MeCab.Tagger()\n",
        "token_list = [[token for token in sent.split()] for sent in train_xs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kHqY4WkBhe7U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fasttext_model = FastText(size=300)\n",
        "fasttext_model.build_vocab(token_list)\n",
        "fasttext_model.train(token_list, total_examples=fasttext_model.corpus_count,epochs=fasttext_model.epochs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XNhGlGYVtme6",
        "colab_type": "code",
        "outputId": "e98bc64b-d9b8-4557-8455-6e1bf347e496",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "train_xs_ = np.array([sent2vec(fasttext_model, sent) for sent in train_xs])\n",
        "test_xs_ = np.array([sent2vec(fasttext_model, sent) for sent in test_xs])\n",
        "model = GradientBoostingClassifier(random_state=23)\n",
        "model.fit(train_xs_, train_ys)\n",
        "print(classification_report(test_ys, model.predict(test_xs_)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.61      0.61      0.61       127\n",
            "           1       0.35      0.34      0.34       118\n",
            "           2       0.62      0.78      0.69       112\n",
            "           3       0.21      0.19      0.20        69\n",
            "           4       0.67      0.73      0.70       101\n",
            "           5       0.30      0.31      0.30       126\n",
            "           6       0.73      0.75      0.74       121\n",
            "           7       0.50      0.48      0.49       132\n",
            "           8       0.48      0.34      0.40        94\n",
            "\n",
            "    accuracy                           0.52      1000\n",
            "   macro avg       0.50      0.50      0.50      1000\n",
            "weighted avg       0.51      0.52      0.51      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1zwinWUXWaF",
        "colab_type": "text"
      },
      "source": [
        "## 改善１：Epoch数調整"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XJqJKbYPV1jX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fasttext_model = FastText(size=300)\n",
        "fasttext_model.build_vocab(token_list)\n",
        "fasttext_model.train(token_list, total_examples=fasttext_model.corpus_count,epochs=20, random=23)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z3IlNP3_WN6d",
        "colab_type": "code",
        "outputId": "f2b5b217-1d3e-4361-9f07-e3624f5b46e2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "train_xs_ = np.array([sent2vec(fasttext_model, sent) for sent in train_xs])\n",
        "test_xs_ = np.array([sent2vec(fasttext_model, sent) for sent in test_xs])\n",
        "model = GradientBoostingClassifier(random_state=23)\n",
        "model.fit(train_xs_, train_ys)\n",
        "print(classification_report(test_ys, model.predict(test_xs_)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.74      0.73       127\n",
            "           1       0.57      0.49      0.53       118\n",
            "           2       0.78      0.78      0.78       112\n",
            "           3       0.34      0.35      0.35        69\n",
            "           4       0.67      0.68      0.68       101\n",
            "           5       0.44      0.50      0.47       126\n",
            "           6       0.75      0.80      0.77       121\n",
            "           7       0.68      0.64      0.66       132\n",
            "           8       0.66      0.61      0.63        94\n",
            "\n",
            "    accuracy                           0.63      1000\n",
            "   macro avg       0.62      0.62      0.62      1000\n",
            "weighted avg       0.64      0.63      0.63      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOI6o8ZZL355",
        "colab_type": "text"
      },
      "source": [
        "## 改善２：Window size調整"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h0mzoE-n3qrb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "window_size = 8\n",
        "fasttext_model = FastText(size=300,window=window_size)\n",
        "fasttext_model.build_vocab(token_list)\n",
        "fasttext_model.train(token_list, total_examples=fasttext_model.corpus_count,epochs=20,random=23)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9xvPRVYMDCN",
        "colab_type": "code",
        "outputId": "2112692f-38af-4aa6-8410-083ae0f77a26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "train_xs_ = np.array([sent2vec(fasttext_model, sent) for sent in train_xs])\n",
        "test_xs_ = np.array([sent2vec(fasttext_model, sent) for sent in test_xs])\n",
        "model = GradientBoostingClassifier(random_state=23)\n",
        "model.fit(train_xs_, train_ys)\n",
        "print(classification_report(test_ys, model.predict(test_xs_)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.74      0.75       127\n",
            "           1       0.62      0.49      0.55       118\n",
            "           2       0.81      0.81      0.81       112\n",
            "           3       0.32      0.36      0.34        69\n",
            "           4       0.69      0.76      0.72       101\n",
            "           5       0.50      0.53      0.51       126\n",
            "           6       0.76      0.83      0.80       121\n",
            "           7       0.71      0.74      0.72       132\n",
            "           8       0.76      0.60      0.67        94\n",
            "\n",
            "    accuracy                           0.67      1000\n",
            "   macro avg       0.66      0.65      0.65      1000\n",
            "weighted avg       0.67      0.67      0.67      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EnjklyjZXkm",
        "colab_type": "text"
      },
      "source": [
        "## 改善３：名詞動詞のみ考慮する\n",
        "ノーズ除去とWindow size拡大の効果がある。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3BPU3sPBuXSj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_noun_verb(parser, doc, typ=[\"名詞\", \"動詞\"]):\n",
        "    parsed = parser.parse(doc).strip()\n",
        "    results = parsed.split(\"\\n\")[:-1]\n",
        "    word_list = []\n",
        "    for parse_result in results:\n",
        "        word = parse_result.split(\"\\t\")[0]\n",
        "        result = parse_result.split(\"\\t\")[1].split(\",\")\n",
        "        pos = result[0]\n",
        "        origin = result[6]\n",
        "        if pos in typ:\n",
        "            if pos == \"名詞\":\n",
        "                word_list.append(word)\n",
        "            else:\n",
        "                word_list.append(origin)\n",
        "    return word_list"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGCXZmn-w2b2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_xs = train_df[:500][\"text\"].apply(lambda x: get_noun_verb(paser, x))\n",
        "test_xs = train_df[2000:3000][\"text\"].apply(lambda x: get_noun_verb(paser, x))\n",
        "token_list = [[token for token in sent] for sent in train_xs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1oJt7pf7ael",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fasttext_model = FastText(size=300,window=window_size)\n",
        "fasttext_model.build_vocab(token_list)\n",
        "fasttext_model.train(token_list, total_examples=fasttext_model.corpus_count,epochs=20,random=23)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6J2dpISE0URz",
        "colab_type": "code",
        "outputId": "deb33f2f-7ea4-46d4-ce76-53a3de4cf5cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "train_xs_ = np.array([sent2vec(fasttext_model, sent) for sent in train_xs])\n",
        "test_xs_ = np.array([sent2vec(fasttext_model, sent) for sent in test_xs])\n",
        "model = GradientBoostingClassifier(random_state=23)\n",
        "model.fit(train_xs_, train_ys)\n",
        "print(classification_report(test_ys, model.predict(test_xs_)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.73      0.79      0.76       127\n",
            "           1       0.69      0.56      0.62       118\n",
            "           2       0.72      0.78      0.75       112\n",
            "           3       0.37      0.46      0.41        69\n",
            "           4       0.78      0.82      0.80       101\n",
            "           5       0.69      0.62      0.65       126\n",
            "           6       0.85      0.85      0.85       121\n",
            "           7       0.81      0.88      0.84       132\n",
            "           8       0.81      0.66      0.73        94\n",
            "\n",
            "    accuracy                           0.73      1000\n",
            "   macro avg       0.72      0.71      0.71      1000\n",
            "weighted avg       0.73      0.73      0.73      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JM9iy4w1fXwb",
        "colab_type": "text"
      },
      "source": [
        "## 改善４：Stopwordsを除く"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y2_OfkzexgtT",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "stopwords = \"\"\"する\n",
        "いる\n",
        "れる\n",
        "こと\n",
        "ある\n",
        "なる\n",
        "ため\n",
        "られる\n",
        "よう\n",
        "ころ\n",
        "せる\n",
        "できる\n",
        "出来る\n",
        "しまう\n",
        "あそこ\n",
        "あたり\n",
        "あちら\n",
        "あっち\n",
        "あと\n",
        "あな\n",
        "あなた\n",
        "あれ\n",
        "いくつ\n",
        "いつ\n",
        "いま\n",
        "いや\n",
        "いろいろ\n",
        "うち\n",
        "おおまか\n",
        "おまえ\n",
        "おれ\n",
        "がい\n",
        "かく\n",
        "かたち\n",
        "かやの\n",
        "から\n",
        "がら\n",
        "きた\n",
        "くせ\n",
        "ここ\n",
        "こっち\n",
        "こと\n",
        "ごと\n",
        "こちら\n",
        "ごっちゃ\n",
        "これ\n",
        "これら\n",
        "ごろ\n",
        "さまざま\n",
        "さらい\n",
        "さん\n",
        "しかた\n",
        "しよう\n",
        "すか\n",
        "ずつ\n",
        "すね\n",
        "すべて\n",
        "ぜんぶ\n",
        "そう\n",
        "そこ\n",
        "そちら\n",
        "そっち\n",
        "そで\n",
        "それ\n",
        "それぞれ\n",
        "それなり\n",
        "たくさん\n",
        "たち\n",
        "たび\n",
        "ため\n",
        "だめ\n",
        "ちゃ\n",
        "ちゃん\n",
        "てん\n",
        "とおり\n",
        "とき\n",
        "どこ\n",
        "どこか\n",
        "ところ\n",
        "どちら\n",
        "どっか\n",
        "どっち\n",
        "どれ\n",
        "なか\n",
        "なかば\n",
        "なに\n",
        "など\n",
        "なん\n",
        "はじめ\n",
        "はず\n",
        "はるか\n",
        "ひと\n",
        "ひとつ\n",
        "ふく\n",
        "ぶり\n",
        "べつ\n",
        "へん\n",
        "ぺん\n",
        "ほう\n",
        "ほか\n",
        "まさ\n",
        "まし\n",
        "まとも\n",
        "まま\n",
        "みたい\n",
        "みつ\n",
        "みなさん\n",
        "みんな\n",
        "もと\n",
        "もの\n",
        "もん\n",
        "やつ\n",
        "よう\n",
        "よそ\n",
        "わけ\n",
        "わたし\n",
        "年\n",
        "月\n",
        "日\n",
        "時\n",
        "分\n",
        "秒\n",
        "週\n",
        "火\n",
        "水\n",
        "木\n",
        "金\n",
        "土\n",
        "国\n",
        "都\n",
        "道\n",
        "府\n",
        "県\n",
        "市\n",
        "区\n",
        "町\n",
        "村\"\"\".split(\"\\n\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bi6g336jaM8a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_noun_verb(parser, doc, typ=[\"名詞\", \"動詞\"]):\n",
        "    parsed = parser.parse(doc).strip()\n",
        "    results = parsed.split(\"\\n\")[:-1]\n",
        "    word_list = []\n",
        "    for parse_result in results:\n",
        "        word = parse_result.split(\"\\t\")[0]\n",
        "        result = parse_result.split(\"\\t\")[1].split(\",\")\n",
        "        pos = result[0]\n",
        "        origin = result[6]\n",
        "        if pos in typ:\n",
        "            if pos == \"名詞\":\n",
        "                word_list.append(word)\n",
        "            else:\n",
        "                word_list.append(origin)\n",
        "    return [word for word in word_list if word not in stopwords] # ここでStopwordsを除く"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GslrUnA2aqOB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_xs = train_df[:500][\"text\"].apply(lambda x: get_noun_verb(paser, x))\n",
        "test_xs = train_df[2000:3000][\"text\"].apply(lambda x: get_noun_verb(paser, x))\n",
        "token_list = [[token for token in sent] for sent in train_xs]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "60W2Jb1FariD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fasttext_model = FastText(size=300, window=window_size)\n",
        "fasttext_model.build_vocab(token_list)\n",
        "fasttext_model.train(token_list, total_examples=fasttext_model.corpus_count,epochs=20,random=23)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAim7MIJatI0",
        "colab_type": "code",
        "outputId": "1c445744-39d7-47df-9f32-1f291004d916",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "train_xs_ = np.array([sent2vec(fasttext_model, sent) for sent in train_xs])\n",
        "test_xs_ = np.array([sent2vec(fasttext_model, sent) for sent in test_xs])\n",
        "model = GradientBoostingClassifier(random_state=23)\n",
        "model.fit(train_xs_, train_ys)\n",
        "print(classification_report(test_ys, model.predict(test_xs_)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.71      0.78       127\n",
            "           1       0.69      0.75      0.72       118\n",
            "           2       0.83      0.81      0.82       112\n",
            "           3       0.33      0.41      0.36        69\n",
            "           4       0.76      0.79      0.78       101\n",
            "           5       0.58      0.64      0.61       126\n",
            "           6       0.91      0.82      0.86       121\n",
            "           7       0.90      0.86      0.88       132\n",
            "           8       0.79      0.78      0.78        94\n",
            "\n",
            "    accuracy                           0.74      1000\n",
            "   macro avg       0.74      0.73      0.73      1000\n",
            "weighted avg       0.76      0.74      0.75      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vmXpiU6gkL92",
        "colab_type": "text"
      },
      "source": [
        "## 改善５：SIF(smooth inverse frequency)でReweightする\n",
        "SIF(smooth inverse frequency)とは、簡単に言うと、単語ベクトルの加重平均で文ベクトルを作る手法である。単語の出現する頻度が高いほどWeightが小さい。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G8p6K3sCion3",
        "colab_type": "code",
        "cellView": "form",
        "colab": {}
      },
      "source": [
        "#@title\n",
        "# https://towardsdatascience.com/fse-2b1ffa791cf9\n",
        "import numpy as np\n",
        "REAL = np.float32 \n",
        "\n",
        "def sif_embeddings(model,sentences,  alpha=1e-3):\n",
        "    \"\"\"Compute the SIF embeddings for a list of sentences\n",
        "    Parameters\n",
        "    ----------\n",
        "    sentences : list\n",
        "        The sentences to compute the embeddings for\n",
        "    model : `~gensim.models.base_any2vec.BaseAny2VecModel`\n",
        "        A gensim model that contains the word vectors and the vocabulary\n",
        "    alpha : float, optional\n",
        "        Parameter which is used to weigh each individual word based on its probability p(w).\n",
        "    Returns\n",
        "    -------\n",
        "    numpy.ndarray \n",
        "        SIF sentence embedding matrix of dim len(sentences) * dimension\n",
        "    \"\"\"\n",
        "    \n",
        "    vlookup = model.wv.vocab  # Gives us access to word index and count\n",
        "    vectors = model.wv        # Gives us access to word vectors\n",
        "    size = model.vector_size  # Embedding size\n",
        "    \n",
        "    Z = 0\n",
        "    for k in vlookup:\n",
        "        Z += vlookup[k].count # Compute the normalization constant Z\n",
        "    \n",
        "    output = []\n",
        "    \n",
        "    # Iterate all sentences\n",
        "    for s in sentences:\n",
        "        count = 0\n",
        "        v = np.zeros(size, dtype=REAL) # Summary vector\n",
        "        # Iterare all words\n",
        "        for w in s:\n",
        "            if w in vlookup:\n",
        "            # The loop over the the vector dimensions is completely unecessary and extremely slow\n",
        "                v += ( alpha / (alpha + (vlookup[w].count / Z))) * vectors[w]\n",
        "                count += 1\n",
        "                \n",
        "        if count > 0:\n",
        "            for i in range(size):\n",
        "                v[i] *= 1/count\n",
        "        output.append(v)\n",
        "    return np.vstack(output).astype(REAL)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KVZv0mYcitKJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_xs = train_df[:500][\"text\"].apply(lambda x: get_noun_verb(paser, x))\n",
        "test_xs = train_df[2000:3000][\"text\"].apply(lambda x: get_noun_verb(paser, x))\n",
        "\n",
        "train_xs_ = sif_embeddings(fasttext_model, train_xs)\n",
        "test_xs_ = sif_embeddings(fasttext_model, test_xs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yWnX592mi9TI",
        "colab_type": "code",
        "outputId": "9538e842-cd8f-4131-d1ee-29226fd36f6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "\n",
        "model = GradientBoostingClassifier(random_state=23)\n",
        "model.fit(train_xs_, train_ys)\n",
        "print(classification_report(test_ys, model.predict(test_xs_)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.65      0.73       127\n",
            "           1       0.72      0.73      0.73       118\n",
            "           2       0.85      0.79      0.82       112\n",
            "           3       0.37      0.61      0.46        69\n",
            "           4       0.77      0.91      0.83       101\n",
            "           5       0.70      0.60      0.65       126\n",
            "           6       0.85      0.88      0.87       121\n",
            "           7       0.93      0.84      0.88       132\n",
            "           8       0.77      0.73      0.75        94\n",
            "\n",
            "    accuracy                           0.75      1000\n",
            "   macro avg       0.75      0.75      0.75      1000\n",
            "weighted avg       0.77      0.75      0.76      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmBI5fZBBjS_",
        "colab_type": "text"
      },
      "source": [
        "## 改善６：Further Training（Online training)\n",
        "データが追加された時、もしくは、学習済みモデルにFine Tuningする時使う。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "58aPOewB-Tv5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 例えば、新しい500件の文書が追加された。\n",
        "additional_data = train_df[500:1000][\"text\"].apply(lambda x: get_noun_verb(paser, x))\n",
        "token_list = [[token for token in sent] for sent in additional_data]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jIqE98OMArCz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# まずVocabularyをUpdateする必要がある。\n",
        "fasttext_model.build_vocab(token_list, update=True)\n",
        "#　その後は普通の学習\n",
        "fasttext_model.train(token_list, total_examples=fasttext_model.corpus_count,epochs=20,random=23)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zHopMrQUBHN1",
        "colab_type": "code",
        "outputId": "3769bcfa-1819-4237-ab0b-3957578d62ef",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 307
        }
      },
      "source": [
        "train_xs_ = sif_embeddings(fasttext_model, train_xs)\n",
        "test_xs_ = sif_embeddings(fasttext_model, test_xs)\n",
        "model = GradientBoostingClassifier(random_state=23)\n",
        "model.fit(train_xs_, train_ys)\n",
        "print(classification_report(test_ys, model.predict(test_xs_)))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.72      0.79       127\n",
            "           1       0.64      0.70      0.67       118\n",
            "           2       0.78      0.84      0.81       112\n",
            "           3       0.49      0.54      0.51        69\n",
            "           4       0.76      0.93      0.84       101\n",
            "           5       0.81      0.67      0.74       126\n",
            "           6       0.86      0.87      0.86       121\n",
            "           7       0.91      0.85      0.88       132\n",
            "           8       0.72      0.76      0.74        94\n",
            "\n",
            "    accuracy                           0.77      1000\n",
            "   macro avg       0.76      0.76      0.76      1000\n",
            "weighted avg       0.78      0.77      0.77      1000\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Net-dv0vf-e8",
        "colab_type": "text"
      },
      "source": [
        "# Bertでやったらどうなる？\n",
        "To Be Continued..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_NSuSF1sEe1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}